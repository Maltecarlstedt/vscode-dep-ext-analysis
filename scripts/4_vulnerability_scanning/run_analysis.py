import os
import sys
import argparse
import time
from pathlib import Path
from typing import List, Tuple
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed

# Import the existing VulnerabilityAnalysisRunner
# Make sure the original script is in the same directory or adjust the import path
try:
    from run_vulnerability_analysis import VulnerabilityAnalysisRunner
except ImportError:
    print("Error: Cannot import analysis class")
    sys.exit(1)


class MultiDatabaseAnalysisRunner:
    def __init__(self, databases_dir: str, codeql_path: str = "codeql", parallel: bool = False, max_workers: int = None):
        self.databases_dir = Path(databases_dir)
        self.codeql_path = codeql_path
        self.parallel = parallel
        self.max_workers = max_workers or min(4, multiprocessing.cpu_count())

        # Find all CodeQL databases in the directory
        self.databases = self._find_databases()

    def _find_databases(self) -> List[Path]:
        """Find all CodeQL databases in the specified directory"""
        databases = []

        if not self.databases_dir.exists():
            print(f"Error: Directory {self.databases_dir} does not exist")
            return databases

        for item in self.databases_dir.iterdir():
            if item.is_dir():
                # Check if it's a CodeQL database by looking for database marker files
                db_yml = item / "codeql-database.yml"
                db_lock = item / ".dbinfo"

                if db_yml.exists() or db_lock.exists():
                    databases.append(item)
                    print(f"Found database: {item}")
                # Also check for .db extension
                elif item.suffix == '.db':
                    databases.append(item)
                    print(f"Found database: {item}")

        if not databases:
            print(f"No CodeQL databases found in {self.databases_dir}")

        return sorted(databases)

    def _run_analysis_for_database(self, database_path: Path) -> Tuple[str, bool, List[str]]:
        db_name = database_path.name
        print(f"\n{'='*60}")
        print(f"Processing database: {db_name}")
        print(f"{'='*60}")

        try:
            runner = VulnerabilityAnalysisRunner(
                str(database_path)
            )
            success = runner.run_analysis()
            created_files = runner.run_all_queries() if success else []

            return db_name, success, created_files

        except Exception as e:
            print(f"Error processing database {db_name}: {e}")
            return db_name, False, []

    def run_sequential(self) -> Tuple[List[str], List[str]]:
        successful_databases = []
        failed_databases = []
        all_created_files = []

        for i, database_path in enumerate(self.databases, 1):
            print(f"\n{'='*80}")
            print(f"Processing database {i}/{len(self.databases)}: {database_path.name}")
            print(f"{'='*80}")

            db_name, success, created_files = self._run_analysis_for_database(database_path)

            if success:
                successful_databases.append(db_name)
                all_created_files.extend(created_files)
                print(f"Successfully processed {db_name}")
            else:
                failed_databases.append(db_name)
                print(f"Failed to process {db_name}")

        return successful_databases, failed_databases

    def run_parallel(self) -> Tuple[List[str], List[str]]:
        """Run analysis on all databases in parallel"""
        successful_databases = []
        failed_databases = []

        print(f"Running analysis on {len(self.databases)} databases using {self.max_workers} workers")

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_db = {
                executor.submit(self._run_analysis_for_database, db_path): db_path.name
                for db_path in self.databases
            }

            # Process completed tasks
            for future in as_completed(future_to_db):
                db_name = future_to_db[future]
                try:
                    db_name, success, created_files = future.result()
                    if success:
                        successful_databases.append(db_name)
                        print(f"Successfully processed {db_name}")
                    else:
                        failed_databases.append(db_name)
                        print(f"Failed to process {db_name}")
                except Exception as e:
                    failed_databases.append(db_name)
                    print(f"Exception processing {db_name}: {e}")

        return successful_databases, failed_databases

    def run_all_analyses(self) -> bool:
        if not self.databases:
            print("No databases found to process")
            return False

        print(f"Found {len(self.databases)} databases to process")
        print(f"Parallel processing: {'Enabled' if self.parallel else 'Disabled'}")

        if self.parallel:
            print(f"Max workers: {self.max_workers}")

        start_time = time.time()

        # Run analysis
        if self.parallel:
            successful_databases, failed_databases = self.run_parallel()
        else:
            successful_databases, failed_databases = self.run_sequential()

        end_time = time.time()

        # Print final summary
        print(f"\n{'='*80}")
        print(f"FINAL SUMMARY")
        print(f"{'='*80}")
        print(f"Total databases: {len(self.databases)}")
        print(f"Successful: {len(successful_databases)}")
        print(f"Failed: {len(failed_databases)}")
        print(f"Total time: {end_time - start_time:.2f} seconds")

        if successful_databases:
            print(f"\nSuccessful databases:")
            for db in successful_databases:
                print(f" --  {db}")

        if failed_databases:
            print(f"\nFailed databases:")
            for db in failed_databases:
                print(f" --  {db}")

        return len(successful_databases) > 0


def main():
    parser = argparse.ArgumentParser(
        description='Run CodeQL vulnerability analysis on all databases in a directory',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python multi_db_analysis.py /path/to/databases

  python multi_db_analysis.py /path/to/databases --parallel --max-workers 8

  python multi_db_analysis.py /path/to/databases --codeql-path /path/to/codeql
        """
    )

    parser.add_argument('databases_dir', default='/Users/alexanderbrunnegard/plugg/datx05/benchmark-dbs',
                       help='Directory containing CodeQL databases')
    parser.add_argument('--codeql-path',
                       default='codeql',
                       help='Path to CodeQL CLI (default: codeql)')
    parser.add_argument('--parallel',
                       action='store_true',
                       help='Run analysis in parallel (can be faster but uses more resources)')
    parser.add_argument('--max-workers',
                       type=int,
                       help='Maximum number of parallel workers (default: min(4, CPU count))')
    parser.add_argument('--list-only',
                       action='store_true',
                       help='Only list found databases without running analysis')

    args = parser.parse_args()

    # Create the multi-database runner
    runner = MultiDatabaseAnalysisRunner(
        databases_dir=args.databases_dir,
        codeql_path=args.codeql_path,
        parallel=args.parallel,
        max_workers=args.max_workers
    )

    # List databases and exit if requested
    if args.list_only:
        print(f"Found {len(runner.databases)} databases:")
        for db in runner.databases:
            print(f"  - {db}")
        return

    # Run the analysis
    success = runner.run_all_analyses()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
