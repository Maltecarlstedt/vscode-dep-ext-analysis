import os
import csv
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class VulnerabilityData:
    """Represents vulnerability counts for a specific dependency version."""
    directory: str
    vulnerabilities: Dict[str, int]
    total: int

@dataclass
class DependencyMatch:
    """Represents a dependency and how it was matched to vulnerability data."""
    name: str
    version: str
    depth: int
    match_type: str
    vulnerabilities: Dict[str, int]

class VulnerabilityLoader:
    """Handles loading and processing vulnerability data from CSV."""

    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.vulnerability_data: Dict[str, VulnerabilityData] = {}
        self.vulnerability_types: List[str] = []
        self.name_mapping: Dict[str, str] = {}

    def load_vulnerabilities(self) -> None:
        """Load vulnerability data from CSV file."""
        logger.info(f"Loading vulnerability data from {self.csv_path}")

        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            # Extract vulnerability types from header (exclude Directory and Total)
            self.vulnerability_types = [col for col in reader.fieldnames
                                    if col not in ['Directory', 'Total']]

            for row in reader:
                # Normalize directory name by removing _db suffix
                directory = row['Directory']
                if directory.endswith('_db'):
                    directory = directory[:-3]

                # Extract vulnerability counts
                vulnerabilities = {}
                for vuln_type in self.vulnerability_types:
                    count = int(row.get(vuln_type, 0))
                    vulnerabilities[vuln_type] = count

                total = int(row.get('Total', 0))

                # Store both the original key and create name mapping for normalized lookup
                self.vulnerability_data[directory] = VulnerabilityData(
                    directory=directory,
                    vulnerabilities=vulnerabilities,
                    total=total
                )

                # Create normalized name mapping for better matching
                # Extract name part (before @version) and normalize it
                if '@' in directory:
                    name_part = directory.split('@')[0]
                    normalized_name = self.normalize_dependency_name(name_part)
                    if normalized_name != name_part:
                        # Create mapping for the normalized version
                        normalized_key = directory.replace(name_part, normalized_name)
                        self.name_mapping[normalized_key] = directory

        logger.info(f"Loaded {len(self.vulnerability_data)} vulnerability records")
        logger.info(f"Created {len(self.name_mapping)} normalized name mappings")
        logger.info(f"Vulnerability types: {', '.join(self.vulnerability_types)}")

    def normalize_dependency_name(self, name: str) -> str:
        """Normalize dependency names to handle variations."""
        # Convert slashes to hyphens in scoped packages
        if name.startswith('@') and '/' in name:
            parts = name.split('/')
            if len(parts) == 2:
                scope = parts[0]
                package = parts[1]
                normalized = f"{scope}-{package}"
                return normalized

        return name

    def get_vulnerabilities(self, name: str, version: str) -> Tuple[Dict[str, int], str]:
        """Get vulnerabilities for a dependency with fallback logic and name normalization."""
        # Try exact match first
        exact_key = f"{name}@{version}"
        if exact_key in self.vulnerability_data:
            vuln_data = self.vulnerability_data[exact_key]
            return vuln_data.vulnerabilities, "exact"

        # Try with normalized name
        normalized_name = self.normalize_dependency_name(name)
        normalized_exact_key = f"{normalized_name}@{version}"

        # Check if we have a mapping for the normalized key
        if normalized_exact_key in self.name_mapping:
            original_key = self.name_mapping[normalized_exact_key]
            if original_key in self.vulnerability_data:
                vuln_data = self.vulnerability_data[original_key]
                return vuln_data.vulnerabilities, "exact"

        # Try direct lookup with normalized key
        if normalized_exact_key in self.vulnerability_data:
            vuln_data = self.vulnerability_data[normalized_exact_key]
            return vuln_data.vulnerabilities, "exact"

        # Try fallback to latest with original name
        fallback_key = f"{name}@latest"
        if fallback_key in self.vulnerability_data:
            vuln_data = self.vulnerability_data[fallback_key]
            return vuln_data.vulnerabilities, "fallback"

        # Try fallback to latest with normalized name
        normalized_fallback_key = f"{normalized_name}@latest"
        if normalized_fallback_key in self.name_mapping:
            original_key = self.name_mapping[normalized_fallback_key]
            if original_key in self.vulnerability_data:
                vuln_data = self.vulnerability_data[original_key]
                return vuln_data.vulnerabilities, "fallback"

        # Try direct lookup with normalized fallback key
        if normalized_fallback_key in self.vulnerability_data:
            vuln_data = self.vulnerability_data[normalized_fallback_key]
            return vuln_data.vulnerabilities, "fallback"

        # No match found
        return {vuln_type: 0 for vuln_type in self.vulnerability_types}, "missing"

class DependencyWalker:
    """Walks dependency trees and extracts all dependencies with depth information."""
    def __init__(self, vulnerability_loader: VulnerabilityLoader):
        self.vulnerability_loader = vulnerability_loader

    def walk_dependencies(self, dependency_tree: Dict[str, Any], depth: int = 0, visited: Optional[set] = None) -> List[DependencyMatch]:
        """Recursively walk dependency tree and collect all dependencies with their vulnerabilities and depth."""
        if visited is None:
            visited = set()

        matches = []

        # Handle the case where dependency_tree is a list (nested dependencies)
        if isinstance(dependency_tree, list):
            for dep in dependency_tree:
                if isinstance(dep, dict):
                    matches.extend(self._process_dependency(dep, depth, visited))
            return matches

        # Handle the case where dependency_tree is the root dependencies dict
        if isinstance(dependency_tree, dict):
            # Check if this is a single dependency object or a collection
            if "name" in dependency_tree:
                # Single dependency
                matches.extend(self._process_dependency(dependency_tree, depth, visited))
            else:
                # Collection of dependencies
                for dep_key, dep_value in dependency_tree.items():
                    if isinstance(dep_value, dict):
                        matches.extend(self._process_dependency(dep_value, depth, visited))

        return matches

    def _process_dependency(self, dependency: Dict[str, Any], depth: int, visited: set) -> List[DependencyMatch]:
        """Process a single dependency and its children with depth tracking."""
        matches = []

        name = dependency.get("name", "")
        version = dependency.get("version", "unknown")

        if not name:
            return matches

        # Create unique identifier to avoid cycles
        dep_id = f"{name}@{version}@{depth}"  # Include depth to allow same dep at different depths
        if dep_id in visited:
            return matches

        visited.add(dep_id)

        # Get vulnerabilities for this dependency
        vulnerabilities, match_type = self.vulnerability_loader.get_vulnerabilities(name, version)

        # Create dependency match with depth information
        match = DependencyMatch(
            name=name,
            version=version,
            depth=depth,
            match_type=match_type,
            vulnerabilities=vulnerabilities
        )
        matches.append(match)

        # Process nested dependencies at increased depth
        nested_deps = dependency.get("dependencies", [])
        if nested_deps:
            matches.extend(self.walk_dependencies(nested_deps, depth + 1, visited))

        return matches

class ExtensionProcessor:
    """Processes individual extension files and generates vulnerability propagation reports with depth analysis."""

    def __init__(self, vulnerability_loader: VulnerabilityLoader):
        self.vulnerability_loader = vulnerability_loader
        self.dependency_walker = DependencyWalker(vulnerability_loader)

    def process_extension(self, extension_file: str) -> Dict[str, Any]:
        """Process a single extension file and generate vulnerability propagation report with depth information."""
        extension_name = Path(extension_file).stem
        logger.debug(f"Processing extension: {extension_name}")

        try:
            with open(extension_file, 'r', encoding='utf-8') as f:
                extension_data = json.load(f)
        except Exception as e:
            logger.error(f"Error loading {extension_file}: {e}")
            return self._create_empty_report(extension_name, str(e))

        # Extract dependencies
        dependencies = extension_data.get("dependencies", {})
        if not dependencies:
            logger.debug(f"No dependencies found for {extension_name}")
            return self._create_empty_report(extension_name)

        # Walk all dependencies with depth tracking
        all_matches = self.dependency_walker.walk_dependencies(dependencies, depth=0)

        # Aggregate vulnerabilities with depth analysis
        total_vulnerabilities = 0
        vulnerability_breakdown = defaultdict(int)
        dependency_sources = {}
        dependency_vulnerabilities = {}
        depth_analysis = defaultdict(lambda: {
            "vulnerabilities": defaultdict(int),
            "dependencies": [],
            "total_vulnerabilities": 0
        })

        for match in all_matches:
            # Only include dependencies that have vulnerabilities or are successfully matched
            has_vulnerabilities = any(count > 0 for count in match.vulnerabilities.values())
            is_matched = match.match_type in ["exact", "fallback"]

            if has_vulnerabilities and is_matched:
                # Track dependency source
                lookup_key = f"{match.name}@{match.version}"
                if match.match_type == "fallback":
                    lookup_key = f"{match.name}@latest"

                dependency_sources[lookup_key] = match.match_type

                # Track vulnerabilities per dependency with depth
                dep_total = sum(match.vulnerabilities.values())
                dependency_vulnerabilities[lookup_key] = {
                    "total_vulnerabilities": dep_total,
                    "breakdown": {k: v for k, v in match.vulnerabilities.items() if v > 0},
                    "match_type": match.match_type,
                    "depth": match.depth
                }

                # Aggregate vulnerabilities
                for vuln_type, count in match.vulnerabilities.items():
                    if count > 0:
                        vulnerability_breakdown[vuln_type] += count
                        total_vulnerabilities += count

                # Depth analysis
                depth_data = depth_analysis[match.depth]
                depth_data["dependencies"].append({
                    "name": match.name,
                    "version": match.version,
                    "vulnerabilities": {k: v for k, v in match.vulnerabilities.items() if v > 0},
                    "total_vulnerabilities": dep_total,
                    "match_type": match.match_type
                })
                depth_data["total_vulnerabilities"] += dep_total
                for vuln_type, count in match.vulnerabilities.items():
                    if count > 0:
                        depth_data["vulnerabilities"][vuln_type] += count

        # Convert depth analysis to regular dict and calculate summary
        depth_summary = {}
        vulnerabilities_by_depth = {}

        for depth, data in depth_analysis.items():
            depth_summary[str(depth)] = {
                "total_dependencies_with_vulnerabilities": len(data["dependencies"]),
                "total_vulnerabilities": data["total_vulnerabilities"],
                "vulnerability_breakdown": dict(data["vulnerabilities"]),
                "dependencies": data["dependencies"]
            }
            vulnerabilities_by_depth[str(depth)] = data["total_vulnerabilities"]

        # Calculate depth statistics
        max_depth = max(depth_analysis.keys()) if depth_analysis else 0
        depths_with_vulnerabilities = len([d for d in depth_analysis.values() if d["total_vulnerabilities"] > 0])

        # Create vulnerability propagation report with enhanced depth information
        report = {
            "extension": extension_name,
            "total_vulnerabilities": total_vulnerabilities,
            "vulnerability_breakdown": dict(vulnerability_breakdown),
            "dependency_sources": dependency_sources,
            "dependency_vulnerabilities": dependency_vulnerabilities,
            "total_unique_dependencies_in_extension": len(all_matches),
            "match_statistics": self._calculate_match_stats(all_matches),

            # Enhanced depth analysis
            "depth_analysis": {
                "max_depth": max_depth,
                "depths_with_vulnerabilities": depths_with_vulnerabilities,
                "vulnerabilities_by_depth": vulnerabilities_by_depth,
                "detailed_depth_breakdown": depth_summary
            }
        }

        return report

    def _create_empty_report(self, extension_name: str, error: str = None) -> Dict[str, Any]:
        """Create an empty report for extensions with no dependencies or errors."""
        report = {
            "extension": extension_name,
            "total_vulnerabilities": 0,
            "vulnerability_breakdown": {},
            "dependency_sources": {},
            "dependency_vulnerabilities": {},
            "total_unique_dependencies_in_extension": 0,
            "match_statistics": {
                "exact": 0,
                "fallback": 0,
                "dependencies_with_no_vulnerabilities": 0
            },
            "depth_analysis": {
                "max_depth": 0,
                "depths_with_vulnerabilities": 0,
                "vulnerabilities_by_depth": {},
                "detailed_depth_breakdown": {}
            }
        }

        if error:
            report["error"] = error

        return report

    def _calculate_match_stats(self, matches: List[DependencyMatch]) -> Dict[str, int]:
        """Calculate statistics about dependency matching."""
        stats = {"exact": 0, "fallback": 0, "dependencies_with_no_vulnerabilities": 0}

        for match in matches:
            if match.match_type == "missing":
                stats["dependencies_with_no_vulnerabilities"] += 1
            else:
                stats[match.match_type] += 1

        return stats

class VulnerabilityImpactMapper:
    """Main class that orchestrates the vulnerability vulnerability propagation mapping process with depth tracking."""

    def __init__(self, vuln_csv_path: str, extensions_dirs: List[str], output_dir: str):
        self.vuln_csv_path = vuln_csv_path
        self.extensions_dirs = extensions_dirs
        self.output_dir = output_dir

        # Initialize components
        self.vulnerability_loader = VulnerabilityLoader(vuln_csv_path)
        self.extension_processor = ExtensionProcessor(self.vulnerability_loader)

    def run(self) -> None:
        """Run the complete vulnerability vulnerability propagation mapping process with depth analysis."""
        logger.info("Starting vulnerability vulnerability propagation mapping with depth tracking")

        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)

        # Load vulnerability data
        self.vulnerability_loader.load_vulnerabilities()

        # Find all extension JSON files across all directories
        extension_files = []
        for extensions_dir in self.extensions_dirs:
            if not os.path.exists(extensions_dir):
                logger.warning(f"Extensions directory not found: {extensions_dir}")
                continue

            dir_files = list(Path(extensions_dir).glob("*.json"))
            logger.info(f"Found {len(dir_files)} extension files in {extensions_dir}")
            extension_files.extend(dir_files)

        logger.info(f"Found {len(extension_files)} total extension files to process")

        if not extension_files:
            logger.warning(f"No JSON files found in any of the specified directories")
            return

        # Process each extension
        summary_stats = {
            "total_extensions": len(extension_files),
            "extensions_with_vulnerabilities": 0,
            "total_vulnerabilities_found": 0,
            "match_statistics": {"exact": 0, "fallback": 0, "dependencies_with_no_vulnerabilities": 0},
            "depth_statistics": {
                "max_depth_across_all_extensions": 0,
                "average_max_depth": 0,
                "extensions_with_deep_vulnerabilities": 0,  # vulnerabilities at depth > 2
                "vulnerabilities_by_depth_aggregated": defaultdict(int)
            }
        }

        depth_stats_accumulator = []

        for extension_file in extension_files:
            try:
                report = self.extension_processor.process_extension(extension_file)

                # Update summary statistics
                if report["total_vulnerabilities"] > 0:
                    summary_stats["extensions_with_vulnerabilities"] += 1
                    summary_stats["total_vulnerabilities_found"] += report["total_vulnerabilities"]

                # Update match statistics
                match_stats = report.get("match_statistics", {})
                for match_type, count in match_stats.items():
                    if match_type == "dependencies_with_no_vulnerabilities":
                        summary_stats["match_statistics"]["dependencies_with_no_vulnerabilities"] += count
                    else:
                        summary_stats["match_statistics"][match_type] += count

                # Update depth statistics
                depth_analysis = report.get("depth_analysis", {})
                max_depth = depth_analysis.get("max_depth", 0)
                depth_stats_accumulator.append(max_depth)

                if max_depth > summary_stats["depth_statistics"]["max_depth_across_all_extensions"]:
                    summary_stats["depth_statistics"]["max_depth_across_all_extensions"] = max_depth

                # Check for deep vulnerabilities (depth > 2)
                vulnerabilities_by_depth = depth_analysis.get("vulnerabilities_by_depth", {})
                has_deep_vulnerabilities = any(
                    int(depth) > 2 and count > 0
                    for depth, count in vulnerabilities_by_depth.items()
                )
                if has_deep_vulnerabilities:
                    summary_stats["depth_statistics"]["extensions_with_deep_vulnerabilities"] += 1

                # Aggregate vulnerabilities by depth across all extensions
                for depth_str, vuln_count in vulnerabilities_by_depth.items():
                    summary_stats["depth_statistics"]["vulnerabilities_by_depth_aggregated"][depth_str] += vuln_count

                # Write individual report
                output_file = os.path.join(self.output_dir, f"{report['extension']}_impact.json")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2)

                logger.debug(f"Generated report for {report['extension']}: "
                           f"{report['total_vulnerabilities']} vulnerabilities found, "
                           f"max depth: {max_depth}")

            except Exception as e:
                logger.error(f"Error processing {extension_file}: {e}")

        # Calculate average max depth
        if depth_stats_accumulator:
            summary_stats["depth_statistics"]["average_max_depth"] = sum(depth_stats_accumulator) / len(depth_stats_accumulator)

        # Convert defaultdict to regular dict for JSON serialization
        summary_stats["depth_statistics"]["vulnerabilities_by_depth_aggregated"] = dict(
            summary_stats["depth_statistics"]["vulnerabilities_by_depth_aggregated"]
        )

        # Write summary report with directory and depth information
        summary_stats["directories_processed"] = [
            {
                "directory": ext_dir,
                "exists": os.path.exists(ext_dir),
                "files_found": len(list(Path(ext_dir).glob("*.json"))) if os.path.exists(ext_dir) else 0
            }
            for ext_dir in self.extensions_dirs
        ]

        summary_file = os.path.join(self.output_dir, "vulnerability_propagation_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_stats, f, indent=2)

        # Log final statistics
        logger.info("Vulnerability vulnerability propagation mapping with depth tracking completed")
        logger.info(f"Extensions processed: {summary_stats['total_extensions']}")
        logger.info(f"Extensions with vulnerabilities: {summary_stats['extensions_with_vulnerabilities']}")
        logger.info(f"Total vulnerabilities found: {summary_stats['total_vulnerabilities_found']}")
        logger.info(f"Match statistics: {summary_stats['match_statistics']}")
        logger.info(f"Max depth across all extensions: {summary_stats['depth_statistics']['max_depth_across_all_extensions']}")
        logger.info(f"Average max depth: {summary_stats['depth_statistics']['average_max_depth']:.2f}")
        logger.info(f"Extensions with deep vulnerabilities (>2): {summary_stats['depth_statistics']['extensions_with_deep_vulnerabilities']}")
        logger.info(f"Reports written to: {self.output_dir}")

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(
        description="Create vulnerability vulnerability propagation maps for VS Code extensions with depth tracking"
    )
    parser.add_argument(
        "--vuln-csv",
        required=True,
        help="Path to CSV file containing vulnerability data"
    )
    parser.add_argument(
        "--ext-dirs",
        required=True,
        nargs='+',
        help="One or more directories containing extension JSON files"
    )
    parser.add_argument(
        "--output-dir",
        default="vulnerability_propagation_reports",
        help="Output directory for vulnerability propagation reports (default: vulnerability_propagation_reports)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose logging"
    )

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Validate input paths
    if not os.path.exists(args.vuln_csv):
        logger.error(f"Vulnerability CSV file not found: {args.vuln_csv}")
        return 1

    # Check that at least one extensions directory exists
    valid_dirs = [d for d in args.ext_dirs if os.path.exists(d)]
    if not valid_dirs:
        logger.error(f"None of the specified extension directories exist: {args.ext_dirs}")
        return 1

    if len(valid_dirs) < len(args.ext_dirs):
        missing_dirs = [d for d in args.ext_dirs if d not in valid_dirs]
        logger.warning(f"Some extension directories not found: {missing_dirs}")

    # Run the mapper
    mapper = VulnerabilityImpactMapper(
        vuln_csv_path=args.vuln_csv,
        extensions_dirs=args.ext_dirs,
        output_dir=args.output_dir
    )

    try:
        mapper.run()
        return 0
    except Exception as e:
        logger.error(f"Error during processing: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
